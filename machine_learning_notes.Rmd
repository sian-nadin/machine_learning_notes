# Intro to machine learning
## Supervised learning

## Decision surface
Algorithm takes in data and creates a decision surface that can predict whther future cases will land in a certain category. 
When the decision surface is a straight line it is linear.
![](screenshots/decision_surface.png) 

## Naive Bayes
Use Sci-kit learn Naive Bayes gaussuian function:
```{python}
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) #training data
>>> Y = np.array([1, 1, 1, 2, 2, 2]) # labels
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB() #classifier
>>> clf.fit(X, Y) # fit calssifier with training data and labels
GaussianNB(priors=None)
>>> print(clf.predict([[-0.8, -1]])) # see which label this would get based on classifier
[1] # classifier predicts the points above would be labelled 1
```

### Example: predict how fast/slow to drive given terrain features
We will use Naive Bayes to classify whether we can drive fast or slow given the
terrain features. 
*Evaluate classifier:*
Evaluate calssifier to see how well it's doing at classifying points. The metric we will use is *accuracy*. The accuracy is the (number of points classified correctly(/(total number of points in the set). 
There are two ways we can do this:
(i) Write code that compares predictions to y_test, element by element OR
(ii) Use SKLearn 
```{python}
def NBAccuracy(features_train, labels_train, features_test, labels_test):
    """ compute the accuracy of your Naive Bayes classifier """
    ### import the sklearn module for GaussianNB
    from sklearn.naive_bayes import GaussianNB
    ### create classifier
    clf = GaussianNB()
    ### fit the classifier on the training features and labels
    clf.fit(features_train, labels_train)
    ### use the trained classifier to predict labels for the test features
    pred = clf.predict(features_test)
    ### calculate and return the accuracy on the test data
    from sklearn.metrics import accuracy_score
    accuracy = accuracy_score(labels_test, pred) 
    # Could also do: print clf.score(features_test, labels_test)
    
    return accuracy
```

## Bayes rule
1% of the population have cancer. If we have a test that:
* 90% will be positive if you have cancer
* 90% of the time will be negative if you don't have cancer.
If you get a positive test result what's the probability you have cancer?
![](screenshots/q_chance_of_c.png) 
![](screenshots/chance_of_c.png) 
Both specificity and sensitivity are 90%.  Intuitively, given the test result is 
positive, we know we are in the shaded region (blue and red).  The true positive 
is depicted by red. The answer is 8%

### Prior & posterior
![](screenshots/bayes_rule.png) 
![](screenshots/cancer_1.png) 
Summarising the above in a diagram:
![](screenshots/cancer_2.png)
![](screenshots/cancer_3.png) 

## Bayes rule for Classification
say we have two people _ Chris and Sarah. Tey often talk about love, deal and life in their emails 
Chris talks about:
* love 10% of the time
* deal 80% of the time
* life 10% of the time
Sarah talks about:
* love 50% of the time
* deal 20% of the time
* life 30% of the time
Given this information we can use Naive baye to detremine, based on a random email, who's the likely person who sent this email. 
Say we have the email "Life Deal" we ca use Baye's rule to predict who sent this. There's a 50% (prior)probability that either Chris or Sarah sent this email.
Probabilty it was Chris: 0.1 . 0.8 . 0.5 = 0.04
Probability it was Sarah: 0.3 . 0.2 . 0.5 = 0.03
It's likely that Chris sent the email. 















