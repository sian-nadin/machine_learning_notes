# Intro to machine learning
## Supervised learning

## Decision surface
Algorithm takes in data and creates a decision surface that can predict whther future cases will land in a certain category. 
When the decision surface is a straight line it is linear.
![](screenshots/decision_surface.png) 

## Naive Bayes
Use Sci-kit learn Naive Bayes gaussuian function:
```{python}
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) #training data
>>> Y = np.array([1, 1, 1, 2, 2, 2]) # labels
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB() #classifier
>>> clf.fit(X, Y) # fit calssifier with training data and labels
GaussianNB(priors=None)
>>> print(clf.predict([[-0.8, -1]])) # see which label this would get based on classifier
[1] # classifier predicts the points above would be labelled 1
```

### Example: predict how fast/slow to drive given terrain features
We will use Naive Bayes to classify whether we can drive fast or slow given the
terrain features. 
*Evaluate classifier:*
Evaluate calssifier to see how well it's doing at classifying points. The metric we will use is *accuracy*. The accuracy is the (number of points classified correctly(/(total number of points in the set). 
There are two ways we can do this:
(i) Write code that compares predictions to y_test, element by element OR
(ii) Use SKLearn 
```{python}
def NBAccuracy(features_train, labels_train, features_test, labels_test):
    """ compute the accuracy of your Naive Bayes classifier """
    ### import the sklearn module for GaussianNB
    from sklearn.naive_bayes import GaussianNB
    ### create classifier
    clf = GaussianNB()
    ### note the amount of time to fit data
    t0 = time()
    ### fit the classifier on the training features and labels
    clf.fit(features_train, labels_train)
    print "training time:", round(time()-t0, 3), "s"
    
    t0 = time()
    ### use the trained classifier to predict labels for the test features
    pred = clf.predict(features_test)
    print "prediction time:", round(time()-t0, 3), "s"
    ### calculate and return the accuracy on the test data
    from sklearn.metrics import accuracy_score
    accuracy = accuracy_score(labels_test, pred) 
    # Could also do: print clf.score(features_test, labels_test)
    
    return accuracy
```

## Bayes rule
1% of the population have cancer. If we have a test that:
* 90% will be positive if you have cancer
* 90% of the time will be negative if you don't have cancer.
If you get a positive test result what's the probability you have cancer?
![](screenshots/q_chance_of_c.png) 
![](screenshots/chance_of_c.png) 
Both specificity and sensitivity are 90%.  Intuitively, given the test result is 
positive, we know we are in the shaded region (blue and red).  The true positive 
is depicted by red. The answer is 8%

### Prior & posterior
![](screenshots/bayes_rule.png) 
![](screenshots/cancer_1.png) 
Summarising the above in a diagram:
![](screenshots/cancer_2.png)
![](screenshots/cancer_3.png) 

## Bayes rule for Classification
Say we have two people _ Chris and Sarah. Tey often talk about love, deal and life in their emails 
Chris talks about:
* love 10% of the time
* deal 80% of the time
* life 10% of the time
Sarah talks about:
* love 50% of the time
* deal 20% of the time
* life 30% of the time
Given this information we can use Naive baye to detremine, based on a random email, who's the likely person who sent this email. 
Say we have the email "Life Deal" we ca use Baye's rule to predict who sent this. There's a 50% (prior)probability that either Chris or Sarah sent this email.
(Joint)Probabilty it was Chris: 0.1 . 0.8 . 0.5 = 0.04
(Joint)Probability it was Sarah: 0.3 . 0.2 . 0.5 = 0.03
The posterior probability for Chris is: 0.04/0.07 = 0.57 
The posterior probability for Sarah is: 0.03/0.07 = 0.43

## Pros & cons of Naive Bayes:
* This method ignores the order of words which is actually quite imporatnat in text. It doesn't really understand the text, it just looks at word frequency to do the classification.
* However, it's really easy to implement and simple to run. 
* One particular feature of Naive Bayes is that it’s a good algorithm for working 
with text classification. When dealing with text, it’s very common to treat each unique 
word as a feature, and since the typical person’s vocabulary is many thousands of words, 
this makes for a large number of features. The relative simplicity of the algorithm and the 
independent features assumption of Naive Bayes make it a strong performer for classifying texts. 
* Naive Bayes wouldn't be as good at phrases. for example, if you were ooking up the team Chicago Bulls you don't want results of just bulls or the city of Chicago so it wouldn't work well in a case like this. 
* So make sure if you're going to use it that it suits the question you're going to ask and test it.

## Naive Bayes Mini project
















